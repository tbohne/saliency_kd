{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T14:22:15.857847Z",
     "start_time": "2025-01-28T14:22:15.252178Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesResampler, TimeSeriesScalerMeanVariance\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "from oscillogram_classification.cam import gen_heatmap_dictionary, plot_heatmaps_as_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a4f6f-03ec-48a3-b93c-26a7b2f70fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(path: str) -> Tuple[List[int], List[List[float]]]:\n",
    "    # dataframe containing all signals from the dataset + labels in col 0\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=None, na_values=['-∞', '∞'])\n",
    "    labels = df.iloc[:, 0].tolist()\n",
    "    samples = df.iloc[:, 1:].values.tolist()\n",
    "    return labels, samples\n",
    "\n",
    "def z_normalize_time_series(series: List[float]) -> np.ndarray:\n",
    "    return (series - np.mean(series)) / np.std(series)\n",
    "\n",
    "def plot_signals(signals: np.ndarray, figsize: Tuple[int, int]) -> None:\n",
    "    fig, axs = plt.subplots(len(signals), figsize=figsize)\n",
    "    for signal_idx, signal in enumerate(signals):\n",
    "        axs[signal_idx].plot(signal)\n",
    "        axs[signal_idx].set_title(\"sample \" + str(signal_idx))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data_vis.svg\", format=\"svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def resample(signals: np.ndarray, znorm: bool, target_len: int) -> np.ndarray:\n",
    "    assert signals.shape == (362, 1250)\n",
    "    print(\"target len\", target_len)\n",
    "    # reshape to (n_ts, sz, d)\n",
    "    signals = signals.reshape((signals.shape[0], signals.shape[1], 1))\n",
    "    # downsample\n",
    "    signals_processed = TimeSeriesResampler(sz=target_len).fit_transform(signals)\n",
    "    if znorm:\n",
    "        # z-normalize each sample individually\n",
    "        signals_processed = TimeSeriesScalerMeanVariance().fit_transform(signals_processed)\n",
    "        \n",
    "    assert signals_processed.shape == (362, target_len, 1)\n",
    "    return signals_processed.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d6a1a-d4be-42f6-a69b-65fbd74e38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_signals(path: str, labels: List[int], samples: List[List[float]]):\n",
    "    df = pd.DataFrame(samples)\n",
    "    df.insert(0, \"label\", labels)  # insert labels as first col\n",
    "    df.to_csv(path, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633794e1-ff25-4dbf-aeec-d975d06fe456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a binary classification dataset for EOGVerticalSignal -- first basic approach (!)\n",
    "\n",
    "train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN.tsv\"\n",
    "train_labels, train_samples = load_signals(train_data)\n",
    "train_labels = [i - 1 for i in train_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-10 as regular (0) and 11 as anomaly (1)\n",
    "train_labels = [0 if label <= 10 else 1 for label in train_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TRAIN_BINARY.tsv', train_labels, train_samples)\n",
    "\n",
    "test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST.tsv\"\n",
    "test_labels, test_samples = load_signals(test_data)\n",
    "test_labels = [i - 1 for i in test_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-10 as regular (0) and 11 as anomaly (1)\n",
    "test_labels = [0 if label <= 10 else 1 for label in test_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TEST_BINARY.tsv', test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6af8c-ae48-4170-b375-9b576ec0acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a binary classification dataset for EOGVerticalSignal -- improved label subsumption (!)\n",
    "\n",
    "train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN.tsv\"\n",
    "train_labels, train_samples = load_signals(train_data)\n",
    "train_labels = [i - 1 for i in train_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-5 as regular (0) and 6-11 as anomaly (1)\n",
    "train_labels = [0 if label <= 5 else 1 for label in train_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TRAIN_BINARY.tsv', train_labels, train_samples)\n",
    "\n",
    "test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST.tsv\"\n",
    "test_labels, test_samples = load_signals(test_data)\n",
    "test_labels = [i - 1 for i in test_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-5 as regular (0) and 6-11 as anomaly (1)\n",
    "test_labels = [0 if label <= 5 else 1 for label in test_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TEST_BINARY.tsv', test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train_data = \"Lightning7/Lightning7_TRAIN.tsv\"\n",
    "# test_data = \"Lightning7/Lightning7_TEST.tsv\"\n",
    "\n",
    "#train_data = \"Plane/Plane_TRAIN.tsv\"\n",
    "#test_data = \"Plane/Plane_TEST.tsv\"\n",
    "\n",
    "#train_data = \"MelbournePedestrian/MelbournePedestrian_TRAIN.tsv\"\n",
    "#test_data = \"MelbournePedestrian/MelbournePedestrian_TEST.tsv\"\n",
    "\n",
    "#train_data = \"SemgHandSubjectCh2/SemgHandSubjectCh2_TRAIN.tsv\"\n",
    "#test_data = \"SemgHandSubjectCh2/SemgHandSubjectCh2_TEST.tsv\"\n",
    "\n",
    "#train_data = \"EthanolLevel/EthanolLevel_TRAIN.tsv\"\n",
    "#test_data = \"EthanolLevel/EthanolLevel_TEST.tsv\"\n",
    "\n",
    "#train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN.tsv\"\n",
    "multiclass_test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST.tsv\"\n",
    "train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN_BINARY.tsv\"\n",
    "test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST_BINARY.tsv\"\n",
    "\n",
    "original_labels, _ = load_signals(multiclass_test_data)\n",
    "original_labels = [i - 1 for i in original_labels]\n",
    "print(\"ori:\", original_labels)\n",
    "\n",
    "train_labels, train_signals = load_signals(train_data)\n",
    "print(\"#samples (train):\", len(train_signals))\n",
    "print(\"sample len:\", len(train_signals[0]))\n",
    "print(\"#classes (train):\", len(np.unique(train_labels)))\n",
    "\n",
    "test_labels, test_signals = load_signals(test_data)\n",
    "print(\"\\n#samples (test):\", len(test_signals))\n",
    "print(\"sample len:\", len(test_signals[0]))\n",
    "print(\"#classes (test):\", len(np.unique(test_labels)))\n",
    "\n",
    "if not 0 in train_labels:\n",
    "    train_labels = [i - 1 for i in train_labels]\n",
    "    test_labels = [i - 1 for i in test_labels]\n",
    "\n",
    "print(np.unique(train_labels))\n",
    "\n",
    "print(\n",
    "    \"train\\tclass 0:\", train_labels.count(0),\n",
    "    \"\\tclass 1:\", train_labels.count(1),\n",
    "    \"\\tclass 2:\", train_labels.count(2),\n",
    "    \"\\tclass 3:\", train_labels.count(3),\n",
    "    \"\\tclass 4:\", train_labels.count(4),\n",
    "    \"\\tclass 5:\", train_labels.count(5),\n",
    "    \"\\tclass 6:\", train_labels.count(6),\n",
    ")\n",
    "print(\n",
    "    \"test\\tclass 0:\", test_labels.count(0),\n",
    "    \"\\tclass 1:\", test_labels.count(1),\n",
    "    \"\\tclass 2:\", test_labels.count(2),\n",
    "    \"\\tclass 3:\", test_labels.count(3),\n",
    "    \"\\tclass 4:\", test_labels.count(4),\n",
    "    \"\\tclass 5:\", test_labels.count(5),\n",
    "    \"\\tclass 6:\", test_labels.count(6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling - True -> znorm\n",
    "\n",
    "target_len = 1250 # 1250\n",
    "\n",
    "train_signals = np.array(train_signals)\n",
    "test_signals = np.array(test_signals)\n",
    "\n",
    "assert train_signals.shape == (362, 1250)\n",
    "assert test_signals.shape == (362, 1250)\n",
    "\n",
    "train_signals = resample(train_signals, True, target_len)\n",
    "test_signals = resample(test_signals, True, target_len)\n",
    "\n",
    "print(train_signals.shape)\n",
    "print(test_signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faaf30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vis - one sample for each class\n",
    "\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "one_sample_each = []\n",
    "for label in unique_labels:\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i] == label:\n",
    "            one_sample_each.append(train_signals[i])\n",
    "            break\n",
    "\n",
    "plot_signals(one_sample_each, figsize=(10, len(one_sample_each)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffccef4-3e57-41e7-ba1a-ed6bec331753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis - all samples for one class\n",
    "\n",
    "label = 1\n",
    "all_of_class = []\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i] == label:\n",
    "        all_of_class.append(train_signals[i])\n",
    "\n",
    "plot_signals(all_of_class, figsize=(10, len(all_of_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82695fef-0d9d-4d8e-879f-4c6bc9553784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis - \"avg\" or most representative sample for each class\n",
    "\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.metrics import cdist_dtw\n",
    "\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "avg_sample_each = []\n",
    "for label in unique_labels:\n",
    "    curr_label_signals = []\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i] == label:\n",
    "            curr_label_signals.append(z_normalize_time_series(train_signals[i]))\n",
    "    curr_label_signals = np.array(curr_label_signals)\n",
    "    print(curr_label_signals.shape)\n",
    "\n",
    "    # FIRST APPROACH\n",
    "    # avg_sample_each.append(np.mean(curr_label_signals, axis=0))\n",
    "    # --> not reasonable\n",
    "\n",
    "    # SECOND APPROACH\n",
    "    # tslearn expects shape (n_ts, sz, d)\n",
    "    # data = curr_label_signals[:, :, np.newaxis]\n",
    "    # dba_mean = dtw_barycenter_averaging(data).squeeze().tolist()\n",
    "    # avg_sample_each.append(dba_mean)\n",
    "    # --> not reasonable\n",
    "\n",
    "    # THIRD APPROACH\n",
    "    data = curr_label_signals[:, :, np.newaxis]\n",
    "    # pairwise DTW dist matrix\n",
    "    dtw_matrix = cdist_dtw(data)\n",
    "    \n",
    "    # sample with the smallest avg DTW distance to others\n",
    "    medoid_idx = np.argmin(dtw_matrix.mean(axis=1))\n",
    "    avg_sample_each.append(curr_label_signals[medoid_idx])\n",
    "\n",
    "plot_signals(avg_sample_each, figsize=(10, len(avg_sample_each)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0207cb",
   "metadata": {},
   "source": [
    "## Training with z-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915737ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# deactivate tensorflow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"before:\", train_signals.shape)\n",
    "print(\"before:\", test_signals.shape)\n",
    "\n",
    "num_samples = train_signals.shape[0]\n",
    "sample_len = train_signals.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate -- only if raW\n",
    "print(train_signals.shape)\n",
    "\n",
    "train_signals = train_signals[:, :, np.newaxis]\n",
    "test_signals = test_signals[:, :, np.newaxis]\n",
    "\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"First Sig\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"after:\", train_signals.shape)\n",
    "print(\"after:\", test_signals.shape)\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ebb99",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "- FCN\n",
    "- hyperparameters (`kernel_size, filters, usage of BatchNorm`) found using `KerasTuner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb333aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape: np.ndarray) -> keras.models.Model:\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = build_model(input_shape=train_signals.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedf45",
   "metadata": {},
   "source": [
    "- predefined ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babd622-1f7e-4a62-9c13-37e84159d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oscillogram_classification import models\n",
    "\n",
    "model = models.create_resnet_model(input_shape=train_signals.shape[1:], num_classes=num_classes)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf93f98",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e724f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there should be no model, otherwise retraining!\n",
    "assert not os.path.isfile(\"best_model.keras\")\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1)\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_signals,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model on test data\n",
    "\n",
    "model = keras.models.load_model(\"best_model.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_signals, test_labels)\n",
    "\n",
    "print(\"test acc.:\", test_acc)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss\n",
    "\n",
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2543a3-bc2b-480e-8e31-620eb10aa33e",
   "metadata": {},
   "source": [
    "### GradCAM++ on univariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa7e13-8034-4027-93a7-81976e38d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"tf-keras-gradcam++\"\n",
    "\n",
    "random_index = random.randint(0, len(test_signals) - 1)\n",
    "net_input = test_signals[random_index]\n",
    "assert net_input.shape[1] == 1\n",
    "ground_truth = test_labels[random_index]\n",
    "prediction = model.predict(np.array([net_input]))\n",
    "heatmaps = gen_heatmap_dictionary(method, np.array(net_input), model, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab7333-bc09-40d6-9a0d-006f7e0317fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_as_overlay(heatmaps, net_input, 'test_plot', test_time_values.squeeze()[random_index].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e11f6f",
   "metadata": {},
   "source": [
    "## tsai training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89fb74-987c-4f26-a41a-e6d6707fb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "import sklearn.metrics as skm\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebd170-aaa7-4508-8311-156ff4c2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsai_dataset(train_signals: np.ndarray, train_labels: np.ndarray) -> TSDataLoaders:\n",
    "    # randomly split the indices of the training samples into two sets (train (80%) and validation (20%))\n",
    "    # 'splits' contains a tuple of lists ([train_indices], [validation_indices])\n",
    "    #    - stratify=True -> split the data in such a way that each class's proportion in the train and validation\n",
    "    #      datasets is approximately the same as the proportion in the original dataset\n",
    "    #    - random_state is the seed\n",
    "    splits = get_splits(train_labels, valid_size=.2, stratify=True, random_state=23, shuffle=True)\n",
    "    print(splits)\n",
    "    print(\"--> currently, the above plot wrongly labels 'Valid' as 'Test'\")\n",
    "\n",
    "    # define transformations:\n",
    "    #    - None -> no transformation to the input (X)\n",
    "    #    - Categorize() -> convert labels into categorical format; converts the labels to integers\n",
    "    # my labels are already ints, but I'll leave it here as a more general case\n",
    "    tfms  = [None, [Categorize()]]\n",
    "\n",
    "    # creates tensors to train on, e.g.,\n",
    "    #    dsets[0]: (TSTensor(vars:5, len:500, device=cpu, dtype=torch.float32), TensorCategory(0))\n",
    "    dsets = TSDatasets(train_signals, train_labels, tfms=tfms, splits=splits, inplace=True)\n",
    "\n",
    "    print(\"#train samples:\", len(dsets.train))\n",
    "    print(\"#valid samples:\", len(dsets.valid))\n",
    "\n",
    "    # data loaders: loading data in batches; batch size 64 for training and 128 for validation\n",
    "    #    - TSStandardize: batch normalization\n",
    "    #    - num_workers: 0 -> data loaded in main process\n",
    "    dls = TSDataLoaders.from_dsets(\n",
    "        dsets.train, dsets.valid, bs=[16, 32], batch_tfms=[TSStandardize()], num_workers=0\n",
    "    )\n",
    "    # vis a batch\n",
    "    dls.show_batch(nrows=3, ncols=3, sharey=True)\n",
    "    return dls\n",
    "\n",
    "def train_tsai_model(dls: TSDataLoaders, model: XCM) -> Learner:\n",
    "    # learner encapsulates the data, the model, and other details related to the training process\n",
    "    # -- loss_func=CrossEntropyLossFlat()\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "\n",
    "    # saves curr state of learner (model + weights) to a file named stage0\n",
    "    learn.save('stage0')\n",
    "\n",
    "    # load state of model\n",
    "    learn.load('stage0')\n",
    "\n",
    "    # training over range of learning rates -- find suitable LR (or LR range)\n",
    "    #    - learning rate range where the loss decreases most effectively\n",
    "    learn.lr_find()\n",
    "\n",
    "    # 150 -> num of epochs\n",
    "    #    - involves varying the learning rate in a specific way during training\n",
    "    #    - the cyclical nature helps in faster convergence, avoids getting stuck in local minima,\n",
    "    #      and sometimes achieves better overall performance\n",
    "    #    - it provides a balance between exploring the loss landscape (with higher learning rates)\n",
    "    #    - and exploiting known good areas of the landscape (with lower learning rates)\n",
    "    learn.fit_one_cycle(300, lr_max=1e-3)\n",
    "\n",
    "    learn.save('stage1')\n",
    "    return learn\n",
    "\n",
    "def test_tsai_model(test_signals: np.ndarray, test_labels: np.ndarray, learn: Learner) -> np.float64:\n",
    "    # labeled test data\n",
    "    test_ds = TSDatasets(test_signals, test_labels, tfms=[None, [Categorize()]])\n",
    "    test_dl = dls.valid.new(test_ds)\n",
    "\n",
    "    test_probas, test_targets, test_preds = learn.get_preds(\n",
    "        dl=test_dl, with_decoded=True, save_preds=None, save_targs=None\n",
    "    )    \n",
    "    return skm.accuracy_score(test_targets, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7265c-1b7a-4040-ad39-2be1957c5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsai expects the data in a diff format: (samples, variables, length)\n",
    "\n",
    "# variables = 1 for univariate datasets and >1 for multivariate\n",
    "\n",
    "train_signals = train_signals.reshape(train_signals.shape[0], 1, train_signals.shape[1])\n",
    "test_signals = test_signals.reshape(test_signals.shape[0], 1, test_signals.shape[1])\n",
    "\n",
    "print(train_signals.shape)\n",
    "print(test_signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17b22-10a5-4a53-804e-76cecc8d85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = generate_tsai_dataset(train_signals, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a8d1-9966-46ca-8675-c533738d62e4",
   "metadata": {},
   "source": [
    "## Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfd86b-fe5f-443f-9a3c-1ac1139f662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### models trained on normalized version of RAW TS ###\n",
    "######################################################\n",
    "\n",
    "# creating InceptionTime (is a CNN) model (vars: 5 (5 channels), c: 2 (2 classes))\n",
    "# model = InceptionTime(dls.vars, dls.c)\n",
    "\n",
    "# TODO: InceptionTimePlus\n",
    "\n",
    "# TODO: XceptionTime\n",
    "\n",
    "# TODO: XceptionTimePlus\n",
    "\n",
    "# TODO: OmniScaleCNN\n",
    "\n",
    "# creating XCM\n",
    "# eXplainable Convolutional neural network for Multivariate time series classification (XCM)\n",
    "# model = XCM(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# creating XCMPlus\n",
    "# eXplainable Convolutional neural network for Multivariate time series classification (XCM)\n",
    "model = XCMPlus(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# creating FCN (CNN model)\n",
    "# model = FCN(dls.vars, dls.c)\n",
    "\n",
    "# TODO: FCNPlus\n",
    "\n",
    "# creating ResNet (CNN)\n",
    "# model = ResNet(dls.vars, dls.c)\n",
    "\n",
    "# TODO: ResNetPlus\n",
    "\n",
    "# TODO: XResNet1d\n",
    "\n",
    "# TODO: XResNet1dPlus\n",
    "\n",
    "# TODO: ResCNN\n",
    "\n",
    "# TODO: TCN\n",
    "\n",
    "# creating RNN\n",
    "# model = RNN(dls.vars, dls.c)\n",
    "\n",
    "# creating RNNPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = RNNPlus(dls.vars, dls.c)\n",
    "\n",
    "# TODO: RNNAttention\n",
    "\n",
    "# creating GRU (RNN model)\n",
    "# model = GRU(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = GRUPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUAttention (RNN model + attention)\n",
    "# model = GRUAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating LSTM (RNN model)\n",
    "# model = LSTM(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = LSTMPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMAttention (RNN model + attention)\n",
    "# model = LSTMAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating TSSequencerPlus\n",
    "# model = TSSequencerPlus(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating TransformerModel\n",
    "# model = TransformerModel(dls.vars, dls.c)\n",
    "\n",
    "# TODO: TST\n",
    "\n",
    "# TODO: TSTPlus\n",
    "\n",
    "# TODO: TSPerceiver\n",
    "\n",
    "# TODO: TSiT\n",
    "\n",
    "# TODO: PatchTST\n",
    "\n",
    "# TODO: ROCKETs category\n",
    "\n",
    "# TODO: Wavelet-based NNs category\n",
    "\n",
    "# TODO: Hybrid models category\n",
    "\n",
    "# TODO: Tabular models category\n",
    "\n",
    "#########################################\n",
    "### models trained on feature vectors ###\n",
    "#########################################\n",
    "\n",
    "# TODO: extract + select features, i.e., generate feature vectors\n",
    "\n",
    "# TODO: MLP\n",
    "\n",
    "# TODO: gMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c25c4-006a-4823-87eb-eb9ecd3619e7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff3c1c-d0b5-4bc7-9e35-50ec3266ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there should be no model, otherwise retraining!\n",
    "assert not os.path.isdir(\"export\")\n",
    "assert not os.path.isdir(\"models\")\n",
    "\n",
    "learn = train_tsai_model(dls, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ff55b-f9b1-407c-bb7e-9700ef2ec1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses -> loss development over all epochs for 'train' and 'valid'\n",
    "# final losses ->  zoomed-in view of the final epochs, focusing on loss values towards the end of training\n",
    "# accuracy -> validation accuracy of the model\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657fb3d-32e2-4a93-8bee-ee3d2689640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bb40b-bbed-4fa9-ad13-0d930138da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(nrows=3, ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484ca2b-1c15-42c4-afc7-f78753a41aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_probas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c663c8-5f6e-4476-8829-43f1e0f02a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a82e2-e38f-4836-9e67-6ed68d628bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interested in cases where the model made incorrect predictions at least 3 times\n",
    "confusions = interp.most_confused(min_val=3)\n",
    "for actual_class, pred_class, times in confusions:\n",
    "    print(\"pred:\", pred_class)\n",
    "    print(\"actual:\", actual_class)\n",
    "    print(times, \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eaf07d-6210-40bc-8171-e7149ab71f98",
   "metadata": {},
   "source": [
    "## Inference on additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a52a8-9234-4025-be04-fcc637ed726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_tsai_model(test_signals, test_labels, learn)\n",
    "print(\"test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1045db-1faa-4f32-8bfa-afc12cdc49aa",
   "metadata": {},
   "source": [
    "## GradCAM for XCM and XCMPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399a475-c7d8-4e99-b8d4-53757a992539",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(model) in [tsai.models.XCMPlus.XCMPlus, tsai.models.XCM.XCM]\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "\n",
    "print(yb[0])\n",
    "print(xb[0])\n",
    "\n",
    "model.show_gradcam(xb[0], yb[0], figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cddb87-8bc1-411b-9e9e-29c222522553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the built-in gradcam method creates plots that are sometimes unreadable, it is better to\n",
    "# visualize it with the methods from oscillogram_classification.cam\n",
    "\n",
    "# determine number of signals to be used for saliency map gen\n",
    "num_test_samples = len(test_signals)\n",
    "\n",
    "# convert test data to CUDA tensors\n",
    "xb = torch.tensor(test_signals[:num_test_samples], dtype=torch.float32).to('cuda:0')\n",
    "yb = torch.tensor(test_labels[:num_test_samples], dtype=torch.float32).to('cuda:0')\n",
    "\n",
    "input_data, probabilities, targets, predictions = learn.get_X_preds(xb.cpu(), yb.cpu(), with_input=True)\n",
    "predictions = predictions.strip('][').split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cdd4c-474c-44c8-8446-e42b86187a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gen_heatmaps_overlay_side_by_side_new(cams: Dict, signals: np.ndarray, title: str, time_vals: List[float]) -> None:\n",
    "    \"\"\"\n",
    "    Generates the class activation map (heatmap) side-by-side plot - time series as overlay.\n",
    "    This time we actually have many-to-many, i.e., one heatmap for each signal.\n",
    "\n",
    "    :param cams: dictionary containing the class activation maps to be visualized (+ method names)\n",
    "    :param signals: list of signals\n",
    "    :param title: window title, e.g., recorded vehicle component and classification result\n",
    "    :param time_vals: time values to be visualized on the x-axis\n",
    "    \"\"\"\n",
    "    cols = 8\n",
    "    rows = math.ceil(len(cams) / float(cols))\n",
    "    plt.rcParams[\"figure.figsize\"] = cols * 5, rows * 3\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "    \n",
    "    fig.canvas.set_window_title(title)\n",
    "    # bounding box in data coordinates that the image will fill (left, right, bottom, top)\n",
    "    min_vals = []\n",
    "    max_vals = []\n",
    "    for sig in signals:\n",
    "        min_vals.append(np.min(sig))\n",
    "        max_vals.append(np.max(sig))\n",
    "    extent = [0, time_vals[-1], np.floor(np.min(min_vals)), np.ceil(np.max(max_vals))]\n",
    "    frequency = round(len(signals[0]) / time_vals[-1], 2)\n",
    "    fig.text(0.5, -0.035, \"time (s), %d Hz\" % frequency, ha=\"center\", va=\"center\", fontsize=18)\n",
    "\n",
    "    if len(cams) == 1 or rows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for j in range(rows):\n",
    "        axes[j][0].set_ylabel(\"norm. voltage (V)\", fontsize=18)\n",
    "        for i in range(cols):\n",
    "            axes[j][i].set_xlim(extent[0], extent[1])\n",
    "            axes[j][i].title.set_text(list(cams.keys())[j * cols + i])\n",
    "            axes[j][i].title.set_fontsize(18)\n",
    "            axes[j][i].tick_params(axis='x', labelsize=12)\n",
    "            axes[j][i].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "            # heatmap\n",
    "            axes[j][i].imshow(\n",
    "                cams[list(cams.keys())[j * cols + i]][np.newaxis, :], cmap=\"plasma\", aspect=\"auto\", alpha=.75, extent=extent\n",
    "            )\n",
    "            # data\n",
    "            axes[j][i].plot(time_vals, signals[j * cols + i], '#000000')\n",
    "\n",
    "            # i is not allowed to go off limits in the extra row\n",
    "            if j * cols + i == len(cams) - 1:\n",
    "                break\n",
    "            \n",
    "    plt.tight_layout(pad=1.4)\n",
    "    fig.savefig(title + '.png', dpi=300, transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386d072-6e2f-401d-9b9d-281dad3b49bc",
   "metadata": {},
   "source": [
    "## Gen Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940832-a8fb-40a1-b6c3-53a830382536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_index = random.randint(0, len(xb) - 1)\n",
    "\n",
    "all_attr_maps = {}\n",
    "\n",
    "for idx in range(len(xb)):\n",
    "    if type(model) == tsai.models.XCMPlus.XCMPlus:\n",
    "        att_maps = get_attribution_map(\n",
    "            model,\n",
    "            [model.backbone.conv2dblock, model.backbone.conv1dblock],\n",
    "            xb[idx],\n",
    "            detach=True,\n",
    "            apply_relu=True\n",
    "        )\n",
    "    else:  # XCM\n",
    "        att_maps = get_attribution_map(\n",
    "            model,\n",
    "            [model.conv2dblock, model.conv1dblock],\n",
    "            xb[idx],\n",
    "            detach=True,\n",
    "            apply_relu=True\n",
    "        )\n",
    "    att_maps[0] = (att_maps[0] - att_maps[0].min()) / (att_maps[0].max() - att_maps[0].min())\n",
    "    att_maps[1] = (att_maps[1] - att_maps[1].min()) / (att_maps[1].max() - att_maps[1].min())\n",
    "\n",
    "    all_attr_maps[idx] = att_maps\n",
    "    print(\"Ground truth: \", int(yb[idx]), \" Prediction: \", predictions[idx])\n",
    "\n",
    "var_attr_maps = {\"var. attr. map \" + str(i) + \"(gt: \" + str(int(yb[i])) + \", pr: \" + predictions[i] + \", or:\" + str(original_labels[i]) + \")\": all_attr_maps[i][0].cpu().numpy()[0] for i in range(len(xb))}\n",
    "time_attr_maps = {\"time attr. map \" + str(i): all_attr_maps[i][1].cpu().numpy()[0] for i in range(len(xb))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc1035-3a38-45fe-812c-18f236f4f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_var_attr_maps = {}\n",
    "\n",
    "original_indices_of_used_saliency_maps = []\n",
    "\n",
    "# filter out the NaN heatmaps\n",
    "for i, k in enumerate(var_attr_maps.keys()):\n",
    "    if not math.isnan(var_attr_maps[k][0]):\n",
    "        original_indices_of_used_saliency_maps.append(i)\n",
    "        filtered_var_attr_maps[k] = var_attr_maps[k]\n",
    "\n",
    "saliency_maps = []\n",
    "gt_labels = []\n",
    "# 1D saliency maps\n",
    "for k in filtered_var_attr_maps.keys():\n",
    "    saliency_maps.append(filtered_var_attr_maps[k])\n",
    "    gt_labels.append(int(k.split(\",\")[-1].split(\":\")[-1].replace(\")\", \"\")))\n",
    "\n",
    "saliency_maps = np.array(saliency_maps)\n",
    "print(\"number of heatmaps to cluster:\", len(saliency_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747762a9-28b3-4bf4-909b-5b9c4a85c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from kneed import KneeLocator\n",
    "import joblib\n",
    "\n",
    "def determine_k_with_elbow(saliency_maps: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Performs Euclidean k-means clustering with the provided saliency maps over a range of k values.\n",
    "    The aim is to use the elbow method to find the optimal k.\n",
    "\n",
    "    :param saliency_maps: saliency maps to cluster\n",
    "    :return number of clusters (k)\n",
    "    \"\"\"\n",
    "    inertias = []\n",
    "    k_values = range(3, 30)\n",
    "    \n",
    "    for k in k_values:\n",
    "        km = TimeSeriesKMeans(\n",
    "            n_clusters=k,\n",
    "            n_init=N_INIT,\n",
    "            max_iter=MAX_ITER,\n",
    "            verbose=False,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        km.fit(saliency_maps)\n",
    "        inertias.append(km.inertia_)  # inertia = sum of distances to closest centroid\n",
    "\n",
    "    knee = KneeLocator(k_values, inertias, curve='convex', direction='decreasing')\n",
    "    optimal_k = knee.knee\n",
    "    print(f\"optimal number of clusters: {optimal_k}\")\n",
    "    \n",
    "    plt.plot(k_values, inertias, marker='o')\n",
    "    plt.xlabel(\"number of clusters\")\n",
    "    plt.ylabel(\"inertia\")\n",
    "    plt.title(\"Elbow Method\")\n",
    "    plt.show()\n",
    "    return optimal_k\n",
    "\n",
    "def perform_euclidean_k_means_clustering(\n",
    "    saliency_maps: np.ndarray, k: int, gt_labels: np.ndarray, fig: Figure\n",
    ") -> Tuple[Dict, List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Performs Euclidean k-means clustering with the provided saliency maps.\n",
    "\n",
    "    :param saliency_maps: saliency maps to cluster\n",
    "    :param k: number of clusters\n",
    "    :param gt_labels: ground truth labels of the corresponding input signal\n",
    "    :param fig: matplotlib figure to be extended\n",
    "    :return (ground truth labels per cluster, cluster centroids)\n",
    "    \"\"\"\n",
    "    print(\"Euclidean k-means\")\n",
    "    euclidean_km = TimeSeriesKMeans(\n",
    "        n_clusters=k,\n",
    "        n_init=N_INIT,\n",
    "        max_iter=MAX_ITER,\n",
    "        verbose=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    pred_labels = euclidean_km.fit_predict(saliency_maps)\n",
    "    centroids = euclidean_km.cluster_centers_\n",
    "    ground_truth = plot_results(1, \"Euclidean $k$-means\", euclidean_km, np.array(saliency_maps), gt_labels, pred_labels, fig, k)\n",
    "    #joblib.dump((euclidean_km, pred_labels, ground_truth), 'trained_models/euclidean_km.pkl')  # save model\n",
    "    return ground_truth, centroids\n",
    "\n",
    "def perform_dba_k_means_clustering(\n",
    "    saliency_maps: np.ndarray, k: int, gt_labels: np.ndarray, fig: Figure, model_target: str\n",
    ") -> Tuple[Dict, List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Performs DBA k-means clustering with the provided saliency maps.\n",
    "\n",
    "    :param saliency_maps: saliency maps to cluster\n",
    "    :param k: number of clusters\n",
    "    :param gt_labels: ground truth labels of the corresponding input signal\n",
    "    :param fig: matplotlib figure to be extended\n",
    "    :param model_target: type of input the model is applied to, i.e., 'saliency', 'input' or 'multivariate'\n",
    "    :return (ground truth labels per cluster, cluster centroids, pred_labels)\n",
    "    \"\"\"\n",
    "    print(\"DBA k-means\")\n",
    "    dba_km = TimeSeriesKMeans(\n",
    "        n_clusters=k,\n",
    "        n_init=N_INIT,\n",
    "        max_iter=MAX_ITER,\n",
    "        metric=\"dtw\",\n",
    "        verbose=False,\n",
    "        max_iter_barycenter=MAX_ITER_BARYCENTER,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    pred_labels = dba_km.fit_predict(saliency_maps)\n",
    "    centroids = dba_km.cluster_centers_\n",
    "    ground_truth_per_cluster = plot_results(1 + k, \"DBA $k$-means\", dba_km, np.array(saliency_maps), gt_labels, pred_labels, fig, k)\n",
    "    joblib.dump(\n",
    "        (dba_km, pred_labels, ground_truth_per_cluster, centroids, multivariate_test_signals_filtered),\n",
    "        'trained_models/dba_km_' + model_target + '.pkl'\n",
    "    )  # save model to file\n",
    "    return ground_truth_per_cluster, centroids, pred_labels\n",
    "\n",
    "def perform_soft_dtw_k_means_clustering(\n",
    "    saliency_maps: np.ndarray, k: int, gt_labels: np.ndarray, fig: Figure\n",
    ") -> Tuple[Dict, List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Performs Soft-DTW k-means clustering with the provided saliency maps.\n",
    "\n",
    "    :param saliency_maps: saliency maps to cluster\n",
    "    :param k: number of clusters\n",
    "    :param gt_labels: ground truth labels of the corresponding input signal\n",
    "    :param fig: matplotlib figure to be extended\n",
    "    :return (ground truth labels per cluster, cluster centroids)\n",
    "    \"\"\"\n",
    "    print(\"Soft-DTW k-means\")\n",
    "    sdtw_km = TimeSeriesKMeans(\n",
    "        n_clusters=k,\n",
    "        n_init=N_INIT,\n",
    "        max_iter=MAX_ITER,\n",
    "        metric=\"softdtw\",\n",
    "        metric_params={\"gamma\": .01},\n",
    "        verbose=True,\n",
    "        max_iter_barycenter=MAX_ITER_BARYCENTER,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    pred_labels = sdtw_km.fit_predict(saliency_maps)\n",
    "    centroids = sdtw_km.cluster_centers_\n",
    "    ground_truth = plot_results(\n",
    "        1 + 2 * k, \"Soft-DTW $k$-means\", sdtw_km, np.array(saliency_maps), gt_labels, pred_labels, fig, k\n",
    "    )\n",
    "    return ground_truth, centroids\n",
    "    # joblib.dump((sdtw_km, pred_labels, ground_truth), 'trained_models/sdtw_km.pkl')  # save model to file\n",
    "\n",
    "def plot_results(\n",
    "        offset: int, title: str, clustering: TimeSeriesKMeans, saliency_maps: np.ndarray, gt_labels: np.ndarray,\n",
    "        pred_labels: np.ndarray, fig: plt.Figure, k: int\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Plots the results of the clustering procedure.\n",
    "\n",
    "    :param offset: y-offset for the data to be displayed\n",
    "    :param title: plot title\n",
    "    :param clustering: clustering results\n",
    "    :param saliency_maps: saliency maps that were clustered\n",
    "    :param gt_labels: ground truth labels\n",
    "    :param pred_labels: predictions (cluster assignments)\n",
    "    :param fig: figure to add plot to\n",
    "    :param k: number of clusters\n",
    "    :return: ground truth dictionary\n",
    "    \"\"\"\n",
    "    print(\"#########################################################################################\")\n",
    "    print(\"results for\", title)\n",
    "    print(\"#########################################################################################\")\n",
    "    ground_truth_dict = evaluate_performance(gt_labels, pred_labels, k)\n",
    "    for y in range(k):\n",
    "        ax = fig.add_subplot(3, k, y + offset)\n",
    "        for x in saliency_maps[pred_labels == y]:\n",
    "            # TODO: impl multivariate case\n",
    "            ax.plot(x.ravel(), \"k-\", alpha=.2)\n",
    "        ax.plot(clustering.cluster_centers_[y].ravel(), \"r-\")\n",
    "        # ax.set_xlim(0, x_train.shape[1])\n",
    "        # ax.set_ylim(6, 15)\n",
    "        ax.text(0.55, 0.85, \"Cluster %d\" % y, transform=fig.gca().transAxes)\n",
    "        if y == 0:\n",
    "            plt.title(title)\n",
    "    return ground_truth_dict\n",
    "\n",
    "def evaluate_performance(ground_truth: np.ndarray, predictions: np.ndarray, k: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluates the clustering performance.\n",
    "\n",
    "    :param ground_truth: ground truth labels\n",
    "    :param predictions: predicted labels (clusters)\n",
    "    :param k: number of clusters\n",
    "    :return: ground truth dictionary\n",
    "    \"\"\"\n",
    "    cluster_dict = {i: 0 for i in range(k)}\n",
    "    ground_truth_per_cluster = {i: [] for i in range(k)}\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        cluster_dict[predictions[i]] += 1\n",
    "        ground_truth_per_cluster[predictions[i]].append(ground_truth[i])\n",
    "\n",
    "    # ideal would be (n, n, n, n, n) - equally distributed\n",
    "    print(\"cluster distribution:\", list(cluster_dict.values()))\n",
    "\n",
    "    # each cluster should contain heatmaps with identical labels, you don't know which one, but it must be identical\n",
    "    print(\"ground truth per cluster:\")\n",
    "    for val in ground_truth_per_cluster.values():\n",
    "        print(\"\\t-\", val, \"\\n\")\n",
    "    return ground_truth_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0e74d-b258-476b-a7eb-8921ea30ec42",
   "metadata": {},
   "source": [
    "## Clustering Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91928bf-5ead-43be-bd4c-ea1b0dd30af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INIT = 20 # 50  # ensures stability; avoids bad local minima - default in many packages is ~10\n",
    "MAX_ITER = 500  # more than sufficient for convergence in most cases\n",
    "SEED = 42\n",
    "MAX_ITER_BARYCENTER = 300 # 500  # might drop this to ~100–200 for short sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40159db8-b22d-4fa2-814c-ebc7ff507501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading already 'trained' model\n",
    "import joblib\n",
    "\n",
    "dba_km, pred_labels, gt_labels_per_cluster_multivariate, centroids_multivariate, multivariate_test_signals_filtered = joblib.load(\n",
    "    'trained_models/dba_km_multivariate_RAW_LEN500_EQUALSUBSUMPTION.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab39b5-bc75-45a1-b1f3-46d6b120cc82",
   "metadata": {},
   "source": [
    "## Clustering for Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3702b21-b98a-4106-8966-c67e41586ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = determine_k_with_elbow(saliency_maps)\n",
    "\n",
    "num_classes_ground_truth = len(np.unique(gt_labels))\n",
    "NUM_CLASSES = num_classes_ground_truth\n",
    "print(\"number of classes to cluster (ground truth):\", num_classes_ground_truth)\n",
    "print(\"determined k:\", k)\n",
    "cluster_fig = plt.figure(figsize=(5 * k, 8))\n",
    "\n",
    "# e.g., (n_samples, 500)\n",
    "assert saliency_maps.shape == (len(saliency_maps), 500)\n",
    "\n",
    "# perform_euclidean_k_means_clustering(saliency_maps, k, gt_labels, cluster_fig)\n",
    "gt_labels_per_cluster_saliency, centroids_saliency, pred_labels = perform_dba_k_means_clustering(\n",
    "    saliency_maps, k, gt_labels, cluster_fig, \"saliency\"\n",
    ")\n",
    "# perform_soft_dtw_k_means_clustering(saliency_maps, k, gt_labels, cluster_fig)\n",
    "\n",
    "cluster_fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a9a51-76bb-4fef-9827-a4d6a59c4de0",
   "metadata": {},
   "source": [
    "## Clustering for Input Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bbcfd-d6a4-439b-b46c-c0c2aea2f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signals_filtered = np.array([test_signals[i][0] for i in range(len(test_signals)) if i in original_indices_of_used_saliency_maps])\n",
    "ori_labels_filtered = [original_labels[i] for i in range(len(original_labels)) if i in original_indices_of_used_saliency_maps]\n",
    "\n",
    "k = determine_k_with_elbow(test_signals_filtered)\n",
    "\n",
    "num_classes_ground_truth = len(np.unique(original_labels))\n",
    "print(\"number of classes to cluster (ground truth):\", num_classes_ground_truth)\n",
    "print(\"determined k:\", k)\n",
    "cluster_fig = plt.figure(figsize=(5 * k, 8))\n",
    "\n",
    "# e.g., (n_samples, 500)\n",
    "assert test_signals_filtered.shape == (len(original_indices_of_used_saliency_maps), 500)\n",
    "\n",
    "# perform_euclidean_k_means_clustering(test_signals_filtered, k, ori_labels_filtered, cluster_fig)\n",
    "gt_labels_per_cluster_input, centroids_input, pred_labels = perform_dba_k_means_clustering(\n",
    "    test_signals_filtered, k, ori_labels_filtered, cluster_fig, \"input\"\n",
    ")\n",
    "# perform_soft_dtw_k_means_clustering(test_signals_filtered, k, ori_labels_filtered, cluster_fig)\n",
    "\n",
    "cluster_fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374b99b-09f0-4513-b6f8-96c61f27cb1a",
   "metadata": {},
   "source": [
    "## Multivariate Clustering (Input Samples + Saliency Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3440a67-e40d-4f0a-b507-e9d4df7f5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_test_signals_filtered = np.array(\n",
    "    [[test_signals[i][0], list(var_attr_maps.values())[i]] for i in range(len(test_signals)) if i in original_indices_of_used_saliency_maps]\n",
    ")\n",
    "ori_labels_filtered = [original_labels[i] for i in range(len(original_labels)) if i in original_indices_of_used_saliency_maps]\n",
    "\n",
    "print(\"shape before:\", multivariate_test_signals_filtered.shape)\n",
    "multivariate_test_signals_filtered = multivariate_test_signals_filtered.transpose(0, 2, 1)\n",
    "print(\"shape after:\", multivariate_test_signals_filtered.shape)\n",
    "\n",
    "k = determine_k_with_elbow(multivariate_test_signals_filtered)\n",
    "\n",
    "num_classes_ground_truth = len(np.unique(original_labels))\n",
    "print(\"number of classes to cluster (ground truth):\", num_classes_ground_truth)\n",
    "print(\"determined k:\", k)\n",
    "cluster_fig = plt.figure(figsize=(5 * k, 8))\n",
    "\n",
    "# multivariate, e.g., (n_samples, seq_len, n_vars)\n",
    "assert multivariate_test_signals_filtered.shape == (len(original_indices_of_used_saliency_maps), 1250, 2)\n",
    "\n",
    "# perform_euclidean_k_means_clustering(test_signals_filtered, k, ori_labels_filtered, cluster_fig)\n",
    "gt_labels_per_cluster_multivariate, centroids_multivariate, pred_labels = perform_dba_k_means_clustering(\n",
    "    multivariate_test_signals_filtered, k, ori_labels_filtered, cluster_fig, \"multivariate\"\n",
    ")\n",
    "# perform_soft_dtw_k_means_clustering(test_signals_filtered, k, ori_labels_filtered, cluster_fig)\n",
    "\n",
    "cluster_fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e99f2f-e283-429f-b5ae-831f11045908",
   "metadata": {},
   "source": [
    "## Plotting Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90705ac-3061-469b-8ee1-ed617a17ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(centroids_multivariate))\n",
    "print(centroids_multivariate.shape)\n",
    "\n",
    "heatmaps_final = centroids_multivariate[:, :, 1]\n",
    "signals_final = centroids_multivariate[:, :, 0]\n",
    "\n",
    "assert len(heatmaps_final) == len(signals_final)\n",
    "\n",
    "centroid_heats = {\"centroid \" + str(i): heatmaps_final[i] for i in range(len(heatmaps_final))}\n",
    "\n",
    "gen_heatmaps_overlay_side_by_side_new(\n",
    "    centroid_heats,\n",
    "    signals_final,\n",
    "    'var_attr',\n",
    "    np.array(range(len(list(heatmaps_final)[0])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9baca6-04f5-419b-9e13-ae09ccc48ced",
   "metadata": {},
   "source": [
    "## Quantitative Metrics -- Ground Truth Eval\n",
    "\n",
    "### (for demonstrating the superiority of heatmap-aided clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1766b-7fb0-45c3-8c9c-69c6df885fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, v_measure_score, confusion_matrix\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = confusion_matrix(y_true, y_pred)\n",
    "    return np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "def quantitative_eval_clustering(gt_labels_per_cluster: List[List[int]]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Quantitative evaluation of the clustering results.\n",
    "\n",
    "    :param gt_labels_per_cluster ground truth labels of the samples assigned to each cluster, i.e., clustering results + gt label\n",
    "    :return tuple of three metrics: (ari, nmi, v, purity)\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    cluster_assignments = []\n",
    "\n",
    "    # e.g., clusters[\n",
    "    #   [1, 1, 1, 2, 3, 4, 5, 5, 5],\n",
    "    #   [2, 2, 2, 2, 3, 4, 2, 2, 2]\n",
    "    # ]\n",
    "    # -->\n",
    "    # true labels: [1, 1, 1, 2, 3, 4, 5, 5, 5, 2, 2, 2, 2, 3, 4, 2, 2, 2]\n",
    "    # cluster_ass: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    for cluster_id, label_list in enumerate(gt_labels_per_cluster):\n",
    "        for label in label_list:\n",
    "            true_labels.append(label)\n",
    "            cluster_assignments.append(cluster_id)\n",
    "\n",
    "    assert len(true_labels) == len(cluster_assignments)\n",
    "\n",
    "    # true_labels: List[int], e.g., [0, 0, 1, 3, 4, 4, 4, 5, 6, 2, 2, 2, 1]\n",
    "    #     - this is the ground truth class of each sample\n",
    "    # cluster_assignments: List[int], e.g., same\n",
    "    #     - assigns an ID to each cluster and simply counts how many samples are assigned to it\n",
    "    ari = adjusted_rand_score(true_labels, cluster_assignments)\n",
    "    nmi = normalized_mutual_info_score(true_labels, cluster_assignments)\n",
    "    v = v_measure_score(true_labels, cluster_assignments)\n",
    "    purity = purity_score(true_labels, cluster_assignments)\n",
    "\n",
    "    return ari, nmi, v, purity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48010-5713-478f-9a6c-b8241a49c701",
   "metadata": {},
   "source": [
    "## Quantitative Metrics -- Without Ground Truth\n",
    "\n",
    "### (for demonstrating the actual process in practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0711d5e-fd5c-4b07-9d3f-d7863decf590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw_path_from_metric\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "\n",
    "SAMPLE_LEN = 1250\n",
    "\n",
    "def intra_cluster_dtw_distances(clusters, centroids):  # works for multivariate\n",
    "    intra_dists = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        # default metric \"euclidean\"\n",
    "        dists = [dtw_path_from_metric(ts, centroids[i]) for ts in cluster]\n",
    "        dists = [d[1] for d in dists]\n",
    "        intra_dists.append(round(np.median(dists), 2))\n",
    "    return intra_dists\n",
    "\n",
    "def inter_cluster_dtw_distances(centroids):  # works for multivariate\n",
    "    distance_grid = [[0 for i in range(len(centroids))] for _ in range(len(centroids))]\n",
    "    for c1, c2 in combinations(range(len(centroids)), 2):\n",
    "        # default metric \"euclidean\"\n",
    "        _, dist = dtw_path_from_metric(centroids[c1], centroids[c2])\n",
    "        dist = round(dist, 2)\n",
    "        distance_grid[c1][c2] = dist\n",
    "        distance_grid[c2][c1] = dist\n",
    "    return distance_grid\n",
    "\n",
    "def dtw_silhouette_score(clusters):\n",
    "    all_series = [ts for cluster in clusters for ts in cluster]\n",
    "    labels = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        labels.extend([i] * len(cluster))\n",
    "\n",
    "    scores = []\n",
    "    for idx, ts in enumerate(all_series):\n",
    "        label = labels[idx]\n",
    "        a = np.mean([dtw_path_from_metric(ts, other)[1] for other in clusters[label] if not np.array_equal(ts, other)])\n",
    "\n",
    "        b = np.inf\n",
    "        for j, cluster in enumerate(clusters):\n",
    "            if j == label:\n",
    "                continue\n",
    "            dist = np.mean([dtw_path_from_metric(ts, other)[1] for other in cluster])\n",
    "            b = min(b, dist)\n",
    "\n",
    "        s = (b - a) / max(a, b) if max(a, b) != 0 else 0\n",
    "        scores.append(s)\n",
    "\n",
    "    return round(np.mean(scores), 2)\n",
    "\n",
    "def cluster_size_entropy(clusters):\n",
    "    sizes = np.array([len(c) for c in clusters])\n",
    "    probs = sizes / np.sum(sizes)\n",
    "    return round(entropy(probs), 2)\n",
    "\n",
    "def total_variance(clusters):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        stacked = np.stack(cluster)\n",
    "        # global scalar variance over all features and all samples — compact summary of cluster spread\n",
    "        variances.append(round(np.mean(np.var(stacked)), 2))\n",
    "    return variances\n",
    "\n",
    "def cluster_variance_across_samples(clusters):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        assert cluster.shape == (len(clusters[i]), SAMPLE_LEN, 2)\n",
    "        # for each time step and variable: how much do samples vary? i.e., cluster spread\n",
    "        variances.append(round(np.mean(np.var(cluster, axis=0)), 2))\n",
    "    return variances\n",
    "\n",
    "def cluster_variance_across_time_steps(clusters):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        assert cluster.shape == (len(clusters[i]), SAMPLE_LEN, 2)\n",
    "        # for each sample and variable: how much does the sample fluctuate over time?\n",
    "        variances.append(round(np.mean(np.var(cluster, axis=1)), 2))\n",
    "    return variances\n",
    "\n",
    "def cluster_variance_across_variables(clusters):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        assert cluster.shape == (len(clusters[i]), SAMPLE_LEN, 2)\n",
    "        # for each sample and time step: how different are the variables at that moment?\n",
    "        variances.append(round(np.mean(np.var(cluster, axis=2)), 2))\n",
    "    return variances\n",
    "\n",
    "def intra_class_variance(clusters, centroids):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        assert cluster.shape == (len(clusters[i]), SAMPLE_LEN, 2)\n",
    "        assert centroids[i].shape == (SAMPLE_LEN, 2)\n",
    "        # squared distances of each signal to centroid\n",
    "        squared_diffs = (cluster - centroids[i]) ** 2\n",
    "        # sum over time and variables for each signal\n",
    "        squared_dists = np.sum(squared_diffs, axis=(1, 2))\n",
    "        # avg over all signals\n",
    "        variances.append(round(np.mean(squared_dists), 2))\n",
    "    return variances\n",
    "\n",
    "def cov_mat(clusters):\n",
    "    variances = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        assert cluster.shape == (len(clusters[i]), SAMPLE_LEN, 2)\n",
    "        X_flat = cluster.reshape(-1, 2)\n",
    "        cov_matrix = np.cov(X_flat, rowvar=False)\n",
    "        scalar_covariance = np.mean(cov_matrix)\n",
    "        variances.append(round(scalar_covariance, 2))\n",
    "    return variances\n",
    "\n",
    "\n",
    "clusters = [[] for _ in range(len(centroids_multivariate))]\n",
    "for i, label in enumerate(pred_labels):\n",
    "    clusters[label].append(multivariate_test_signals_filtered[i])\n",
    "\n",
    "intra = intra_cluster_dtw_distances(clusters, centroids_multivariate)\n",
    "inter = inter_cluster_dtw_distances(centroids_multivariate)\n",
    "sil_score = dtw_silhouette_score(clusters)\n",
    "entropy_score = cluster_size_entropy(clusters)\n",
    "\n",
    "tv = total_variance(clusters)\n",
    "cvas = cluster_variance_across_samples(clusters)\n",
    "cvat = cluster_variance_across_time_steps(clusters)\n",
    "cvav = cluster_variance_across_variables(clusters)\n",
    "icv = intra_class_variance(clusters, centroids_multivariate)\n",
    "cm = cov_mat(clusters)\n",
    "\n",
    "print(\"intra:\", intra)\n",
    "print(\"\\ninter:\", inter)\n",
    "print(\"\\nsil_score:\", sil_score)\n",
    "print(\"\\nentropy:\", entropy_score, \"max possible:\", math.log(len(centroids_multivariate), 2))\n",
    "print(\"\\ntotal_variance:\", tv)\n",
    "print(\"\\ncluster_variance_across_samples:\", cvas)\n",
    "print(\"\\ncluster_variance_across_time_steps:\", cvat)\n",
    "print(\"\\ncluster_variance_across_variables:\", cvav)\n",
    "print(\"\\nintra_class_variance:\", icv)\n",
    "print(\"\\ncov_mat:\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a805b2-515f-496e-bf47-7f4516b8b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter things\n",
    "\n",
    "# when using znorm data\n",
    "#INTRA_MAX = 110\n",
    "#VAR_MAX = 1\n",
    "\n",
    "# when using raw data\n",
    "INTRA_MAX = 10000\n",
    "VAR_MAX = 25000\n",
    "\n",
    "sizes = np.array([len(c) for c in clusters])\n",
    "inter_means = [np.median(i) for i in inter]\n",
    "\n",
    "# the smaller the values, the better\n",
    "all_var = [tv[i] + icv[i] / 1000 + cvat[i] + cvav[i] + cvas[i] + cm[i] for i in range(len(tv))]\n",
    "\n",
    "# the smaller this value, the better\n",
    "intra_inter_frac = [round(intra[i] / inter_means[i], 2) for i in range(len(inter))]\n",
    "\n",
    "# the smaller, the better\n",
    "punishing_val = [(intra_inter_frac[i] + all_var[i]) for i in range(len(inter))]\n",
    "print(punishing_val)\n",
    "\n",
    "# thresholds\n",
    "intra_thresh = np.percentile(intra, 50)  # keep clusters below median intra\n",
    "var_thresh = np.percentile(tv, 50)  # keep clusters below median variance\n",
    "size_thresh = 5  # avoid tiny clusters\n",
    "\n",
    "selected = []\n",
    "total_valuations = []\n",
    "for i in range(len(intra)):\n",
    "    each_inter_class_dist_larger_than_intra = True\n",
    "    violation_cnt = 0\n",
    "    for j in range(len(inter[i])):\n",
    "        if i != j and inter[i][j] <= intra[i]:\n",
    "            each_inter_class_dist_larger_than_intra = False\n",
    "            violation_cnt += 1\n",
    "            print(\"inter class distance violation (cluster \" + str(i) + \") - inter:\", inter[i][j], \"intra:\", intra[i])\n",
    "    # relative\n",
    "    # if intra[i] < intra_thresh and variances[i] < var_thresh and sizes[i] >= size_thresh:\n",
    "    # absolute\n",
    "    if intra[i] < INTRA_MAX and tv[i] < VAR_MAX and sizes[i] >= size_thresh and each_inter_class_dist_larger_than_intra and inter_means[i] > intra[i]:\n",
    "        selected.append(i)\n",
    "\n",
    "    total_valuations.append(round(all_var[i] * intra_inter_frac[i] + violation_cnt, 2))\n",
    "\n",
    "print(\"selected clusters:\", selected)\n",
    "print(\"total valuations:\", total_valuations)\n",
    "\n",
    "filtered_clusters = [clusters[i] for i in selected]\n",
    "filtered_centroids = [centroids_multivariate[i] for i in selected]\n",
    "\n",
    "cluster_fig = plt.figure(figsize=(5 * len(selected), 8))\n",
    "\n",
    "for idx, y in enumerate(selected):\n",
    "    ax = cluster_fig.add_subplot(3, len(selected), idx + 1 + len(selected))\n",
    "    cluster = np.array(clusters[y])\n",
    "    print(\"cluster shape:\", cluster.shape)\n",
    "\n",
    "    # it is still multivariate (!)\n",
    "    for i in range(cluster.shape[0]):  # iterate over multivariate samples in cluster\n",
    "        ax.plot(cluster[i, :, 0], \"r-\", alpha=.2)  # signal\n",
    "        ax.plot(cluster[i, :, 1], \"b-\", alpha=.2)  # heatmap\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='r', lw=2, label='signal'),\n",
    "        Line2D([0], [0], color='b', lw=2, label='heatmap')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "    ax.plot(centroids_multivariate[y, :, 0].ravel(), \"r-\")\n",
    "    ax.plot(centroids_multivariate[y, :, 1].ravel(), \"b-\")\n",
    "    ax.text(0.55, 0.85, \"cluster %d\" % y, transform=cluster_fig.gca().transAxes)\n",
    "    if y == 0:\n",
    "        plt.title(\"title\")\n",
    "\n",
    "plt.savefig(\"filtered_clusters.png\")\n",
    "\n",
    "centroid_heats_filtered = {\"centroid \" + str(i): heatmaps_final[i] for i in selected}\n",
    "\n",
    "signals_final_filtered = [signals_final[i] for i in selected]\n",
    "\n",
    "gen_heatmaps_overlay_side_by_side_new(\n",
    "    centroid_heats_filtered,\n",
    "    signals_final_filtered,\n",
    "    'filtered_centroids',\n",
    "    np.array(range(len(list(heatmaps_final)[0])))\n",
    ")\n",
    "\n",
    "## to be provided to LLM\n",
    "\n",
    "centroid_only_fig = plt.figure(figsize=(5 * len(selected), 8))\n",
    "\n",
    "for idx, y in enumerate(selected):\n",
    "    ax = centroid_only_fig.add_subplot(3, len(selected), idx + 1 + len(selected))\n",
    "\n",
    "    ax.plot(signals_final[y], \"r-\")\n",
    "    ax.text(0.55, 0.85, \"centroid %d\" % y, transform=centroid_only_fig.gca().transAxes)\n",
    "\n",
    "plt.savefig(\"centroids4llm.png\")\n",
    "\n",
    "## to be provided to LLM\n",
    "\n",
    "filtered_centroid_signals = []\n",
    "for i in selected:\n",
    "    filtered_centroid_signals.append(signals_final[i])\n",
    "np.save(\"centroids4llm.npy\", np.array(filtered_centroid_signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70348c7-cd45-4bf1-9c61-a28256fb2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## informative for own interpretation\n",
    "\n",
    "print(\"filtered clusters:\")\n",
    "for i in selected:\n",
    "    counts = np.bincount(gt_labels_per_cluster_multivariate[i])\n",
    "    dominant_class = np.argmax(counts)\n",
    "    print(i, \"(dominant\", dominant_class, \"):\", gt_labels_per_cluster_multivariate[i], \"\\n\")\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"all clusters:\")\n",
    "for k in gt_labels_per_cluster_multivariate.keys():\n",
    "    print(k, \":\", gt_labels_per_cluster_multivariate[k], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea0179-6f35-4f4d-83e5-62c302d67ebc",
   "metadata": {},
   "source": [
    "## Eval the 3 Cases - Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87001fb3-ec0c-4df6-9fe5-4df5ab4a4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI: 1.0 -> perfect match; 0.0 -> random labeling; < 0.0 -> worse than random\n",
    "# NMI: 1.0 -> perfect correlation; 0.0 -> no mutual information\n",
    "# V-Measure (a.k.a. conditional entropy): 1.0 -> perfect clustering; 0.0 -> worst case\n",
    "\n",
    "ari, nmi, v, purity = quantitative_eval_clustering(list(gt_labels_per_cluster_saliency.values()))\n",
    "print(\"for saliency maps:\\t\" + f\"ARI: {ari:.3f}, NMI: {nmi:.3f}, V-measure: {v:.3f}, Purity: {purity:.3f}\")\n",
    "\n",
    "ari, nmi, v, purity = quantitative_eval_clustering(list(gt_labels_per_cluster_input.values()))\n",
    "print(\"for input samples:\\t\" + f\"ARI: {ari:.3f}, NMI: {nmi:.3f}, V-measure: {v:.3f}, Purity: {purity:.3f}\")\n",
    "\n",
    "# for both in combination, i.e., multivariate\n",
    "ari, nmi, v, purity = quantitative_eval_clustering(list(gt_labels_per_cluster_multivariate.values()))\n",
    "print(\"for multivariate:\\t\" + f\"ARI: {ari:.3f}, NMI: {nmi:.3f}, V-measure: {v:.3f}, Purity: {purity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dac214-97bc-4d29-be0c-c6114b1eea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example case\n",
    "\n",
    "# assumed k is 2; assumed ground truth num of classes is 3\n",
    "# num of samples: 6\n",
    "\n",
    "test_clustering_res = [\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 2]\n",
    "]\n",
    "print(test_clustering_res)\n",
    "\n",
    "ari, nmi, v, purity = quantitative_eval_clustering(test_clustering_res)\n",
    "print(\"for saliency maps:\\t\" + f\"ARI: {ari:.3f}, NMI: {nmi:.3f}, V-measure: {v:.3f}, Purity: {purity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08c8a9-aacb-4da3-a0b6-8788614ec16e",
   "metadata": {},
   "source": [
    "## Plot Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454bbfb-f800-4f41-8130-87e05de35a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xb.cpu().numpy()[0].flatten().shape == var_attr_maps[list(var_attr_maps.keys())[0]].shape\n",
    "\n",
    "gen_heatmaps_overlay_side_by_side_new(\n",
    "    var_attr_maps,\n",
    "    [xb.cpu().numpy()[i].flatten() for i in range(len(xb))],\n",
    "    'var_attr',\n",
    "    np.array(range(len(xb[0].cpu().numpy()[0])))\n",
    ")\n",
    "gen_heatmaps_overlay_side_by_side_new(\n",
    "    time_attr_maps,\n",
    "    [xb.cpu().numpy()[i].flatten() for i in range(len(xb))],\n",
    "    'time_attr',\n",
    "    np.array(range(len(xb[0].cpu().numpy()[0])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713b109-33ff-4ac6-96ce-60418f7af125",
   "metadata": {},
   "source": [
    "## Cross-validation for tsai training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca23b84-ce0a-40b5-ad96-76b90a61b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "train_test_splits = get_splits(\n",
    "    np.concatenate((train_labels, test_labels), axis=0),\n",
    "    n_splits=k,\n",
    "    valid_size=.2,\n",
    "    stratify=True,\n",
    "    random_state=23,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d39cee-069b-4859-b373-1ba36a939889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = np.concatenate((train_signals, test_signals), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "test_accuracies = []\n",
    "\n",
    "for train_test_split in train_test_splits:\n",
    "    train_split_signals = all_signals[train_test_split[0]]\n",
    "    test_split_signals = all_signals[train_test_split[1]]\n",
    "    train_split_labels = all_labels[train_test_split[0]]\n",
    "    test_split_labels = all_labels[train_test_split[1]]\n",
    "\n",
    "    # the training data will be further split into train and validation\n",
    "    dls = generate_tsai_dataset(train_split_signals, train_split_labels)\n",
    "\n",
    "    model = TransformerModel(dls.vars, dls.c)\n",
    "    learn = train_tsai_model(dls, model)\n",
    "    learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')\n",
    "    test_acc = test_tsai_model(test_split_signals, test_split_labels, learn)\n",
    "    test_accuracies.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f12b0-1026-4dd6-b14e-b922734fd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)\n",
    "print(\"Mean accuracy over all folds: \", np.mean(test_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8b0b1",
   "metadata": {},
   "source": [
    "## Load and apply already trained torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcb6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"../data/Lambdasonde.pth\"\n",
    "model = torch.load(model_path)\n",
    "# ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "tensors = torch.from_numpy(test_signals).float()\n",
    "\n",
    "# iterate over test signals\n",
    "for idx in range(tensors.shape[0]):\n",
    "    # assumes model outputs logits for a multi-class classification problem\n",
    "    logits = model(tensors[[idx]])\n",
    "    # convert logits to probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    print(probabilities)\n",
    "    first_class = float(probabilities[0][0])\n",
    "    second_class = float(probabilities[0][1])\n",
    "\n",
    "    if first_class < second_class:\n",
    "        print(\"pred POS \\t ground truth:\", test_titles[idx])\n",
    "    else:\n",
    "        print(\"pred NEG \\t ground truth:\", test_titles[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
