{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T14:22:15.857847Z",
     "start_time": "2025-01-28T14:22:15.252178Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesResampler\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "from oscillogram_classification.cam import gen_heatmap_dictionary, plot_heatmaps_as_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02464d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(path: str) -> Tuple[List[int], List[List[float]]]:\n",
    "    # dataframe containing all signals from the dataset + labels in col 0\n",
    "    df = pd.read_csv(path, delimiter='\\t', header=None, na_values=['-∞', '∞'])\n",
    "    labels = df.iloc[:, 0].tolist()\n",
    "    samples = df.iloc[:, 1:].values.tolist()\n",
    "    return labels, samples\n",
    "\n",
    "def z_normalize_time_series(series: List[float]) -> np.ndarray:\n",
    "    return (series - np.mean(series)) / np.std(series)\n",
    "\n",
    "def plot_signals(signals: np.ndarray, figsize: Tuple[int, int]) -> None:\n",
    "    fig, axs = plt.subplots(len(signals), figsize=figsize)\n",
    "    for signal_idx, signal in enumerate(signals):\n",
    "        axs[signal_idx].plot(signal)\n",
    "        axs[signal_idx].set_title(\"sample \" + str(signal_idx))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data_vis.svg\", format=\"svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def resample(signals: np.ndarray, znorm: bool) -> np.ndarray:\n",
    "    target_len = 1250\n",
    "    print(\"target len\", target_len)\n",
    "    for i in range(len(signals)):\n",
    "        if len(signals[i]) != target_len:\n",
    "            sig_arr = np.array(signals[i])\n",
    "            sig_arr = sig_arr.reshape((1, len(signals[i]), 1))  # n_ts, sz, d\n",
    "            signals[i] = TimeSeriesResampler(sz=target_len).fit_transform(sig_arr).tolist()[0]\n",
    "        # z-normalization\n",
    "        if znorm:\n",
    "            signals[i] = z_normalize_time_series(signals[i])\n",
    "    return np.array(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633794e1-ff25-4dbf-aeec-d975d06fe456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a binary classification dataset for EOGVerticalSignal\n",
    "\n",
    "def save_signals(path: str, labels: List[int], samples: List[List[float]]):\n",
    "    df = pd.DataFrame(samples)\n",
    "    df.insert(0, \"label\", labels)  # insert labels as first col\n",
    "    df.to_csv(path, sep='\\t', header=False, index=False)\n",
    "\n",
    "train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN.tsv\"\n",
    "train_labels, train_samples = load_signals(train_data)\n",
    "train_labels = [i - 1 for i in train_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-10 as regular (0) and 11 as anomaly (1)\n",
    "train_labels = [0 if label <= 10 else 1 for label in train_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TRAIN_BINARY.tsv', train_labels, train_samples)\n",
    "\n",
    "test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST.tsv\"\n",
    "test_labels, test_samples = load_signals(test_data)\n",
    "test_labels = [i - 1 for i in test_labels]\n",
    "# adjust the labels -- turn into binary classification problem\n",
    "#   - subsume 0-10 as regular (0) and 11 as anomaly (1)\n",
    "test_labels = [0 if label <= 10 else 1 for label in test_labels]\n",
    "save_signals('EOGVerticalSignal/EOGVerticalSignal_TEST_BINARY.tsv', test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train_data = \"Lightning7/Lightning7_TRAIN.tsv\"\n",
    "# test_data = \"Lightning7/Lightning7_TEST.tsv\"\n",
    "\n",
    "#train_data = \"Plane/Plane_TRAIN.tsv\"\n",
    "#test_data = \"Plane/Plane_TEST.tsv\"\n",
    "\n",
    "#train_data = \"MelbournePedestrian/MelbournePedestrian_TRAIN.tsv\"\n",
    "#test_data = \"MelbournePedestrian/MelbournePedestrian_TEST.tsv\"\n",
    "\n",
    "#train_data = \"SemgHandSubjectCh2/SemgHandSubjectCh2_TRAIN.tsv\"\n",
    "#test_data = \"SemgHandSubjectCh2/SemgHandSubjectCh2_TEST.tsv\"\n",
    "\n",
    "#train_data = \"EthanolLevel/EthanolLevel_TRAIN.tsv\"\n",
    "#test_data = \"EthanolLevel/EthanolLevel_TEST.tsv\"\n",
    "\n",
    "train_data = \"EOGVerticalSignal/EOGVerticalSignal_TRAIN.tsv\"\n",
    "test_data = \"EOGVerticalSignal/EOGVerticalSignal_TEST.tsv\"\n",
    "\n",
    "train_labels, train_signals = load_signals(train_data)\n",
    "print(\"#samples (train):\", len(train_signals))\n",
    "print(\"sample len:\", len(train_signals[0]))\n",
    "print(\"#classes (train):\", len(np.unique(train_labels)))\n",
    "\n",
    "test_labels, test_signals = load_signals(test_data)\n",
    "print(\"\\n#samples (test):\", len(test_signals))\n",
    "print(\"sample len:\", len(test_signals[0]))\n",
    "print(\"#classes (test):\", len(np.unique(test_labels)))\n",
    "\n",
    "train_labels = [i - 1 for i in train_labels]\n",
    "test_labels = [i - 1 for i in test_labels]\n",
    "\n",
    "print(np.unique(train_labels))\n",
    "\n",
    "print(\n",
    "    \"train\\tclass 0:\", train_labels.count(0),\n",
    "    \"\\tclass 1:\", train_labels.count(1),\n",
    "    \"\\tclass 2:\", train_labels.count(2),\n",
    "    \"\\tclass 3:\", train_labels.count(3),\n",
    "    \"\\tclass 4:\", train_labels.count(4),\n",
    "    \"\\tclass 5:\", train_labels.count(5),\n",
    "    \"\\tclass 6:\", train_labels.count(6),\n",
    ")\n",
    "print(\n",
    "    \"test\\tclass 0:\", test_labels.count(0),\n",
    "    \"\\tclass 1:\", test_labels.count(1),\n",
    "    \"\\tclass 2:\", test_labels.count(2),\n",
    "    \"\\tclass 3:\", test_labels.count(3),\n",
    "    \"\\tclass 4:\", test_labels.count(4),\n",
    "    \"\\tclass 5:\", test_labels.count(5),\n",
    "    \"\\tclass 6:\", test_labels.count(6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling - True -> znorm\n",
    "train_signals = resample(train_signals, True)\n",
    "test_signals = resample(test_signals, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfaa67-928d-44a0-b26c-06a562f17b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw TS\n",
    "train_signals = np.array(train_signals)\n",
    "test_signals = np.array(test_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faaf30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vis - one sample for each class\n",
    "\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "one_sample_each = []\n",
    "for label in unique_labels:\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i] == label:\n",
    "            one_sample_each.append(train_signals[i])\n",
    "            break\n",
    "\n",
    "plot_signals(one_sample_each, figsize=(10, len(one_sample_each)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffccef4-3e57-41e7-ba1a-ed6bec331753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis - all samples for one class\n",
    "\n",
    "label = 1\n",
    "all_of_class = []\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i] == label:\n",
    "        all_of_class.append(train_signals[i])\n",
    "\n",
    "plot_signals(all_of_class, figsize=(10, len(all_of_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0207cb",
   "metadata": {},
   "source": [
    "## Training with z-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915737ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# deactivate tensorflow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"before:\", train_signals.shape)\n",
    "print(\"before:\", test_signals.shape)\n",
    "\n",
    "num_samples = train_signals.shape[0]\n",
    "sample_len = train_signals.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate -- only if raW\n",
    "print(train_signals.shape)\n",
    "\n",
    "train_signals = train_signals[:, :, np.newaxis]\n",
    "test_signals = test_signals[:, :, np.newaxis]\n",
    "\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"First Sig\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"after:\", train_signals.shape)\n",
    "print(\"after:\", test_signals.shape)\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ebb99",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "- FCN\n",
    "- hyperparameters (`kernel_size, filters, usage of BatchNorm`) found using `KerasTuner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb333aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape: np.ndarray) -> keras.models.Model:\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = build_model(input_shape=train_signals.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdedf45",
   "metadata": {},
   "source": [
    "- predefined ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babd622-1f7e-4a62-9c13-37e84159d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oscillogram_classification import models\n",
    "\n",
    "model = models.create_resnet_model(input_shape=train_signals.shape[1:], num_classes=num_classes)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf93f98",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e724f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there should be no model, otherwise retraining!\n",
    "assert not os.path.isfile(\"best_model.keras\")\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1)\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_signals,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model on test data\n",
    "\n",
    "model = keras.models.load_model(\"best_model.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_signals, test_labels)\n",
    "\n",
    "print(\"test acc.:\", test_acc)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss\n",
    "\n",
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2543a3-bc2b-480e-8e31-620eb10aa33e",
   "metadata": {},
   "source": [
    "### GradCAM++ on univariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa7e13-8034-4027-93a7-81976e38d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"tf-keras-gradcam++\"\n",
    "\n",
    "random_index = random.randint(0, len(test_signals) - 1)\n",
    "net_input = test_signals[random_index]\n",
    "assert net_input.shape[1] == 1\n",
    "ground_truth = test_labels[random_index]\n",
    "prediction = model.predict(np.array([net_input]))\n",
    "heatmaps = gen_heatmap_dictionary(method, np.array(net_input), model, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab7333-bc09-40d6-9a0d-006f7e0317fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_as_overlay(heatmaps, net_input, 'test_plot', test_time_values.squeeze()[random_index].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e11f6f",
   "metadata": {},
   "source": [
    "## tsai training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89fb74-987c-4f26-a41a-e6d6707fb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "import sklearn.metrics as skm\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebd170-aaa7-4508-8311-156ff4c2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsai_dataset(train_signals: np.ndarray, train_labels: np.ndarray) -> TSDataLoaders:\n",
    "    # randomly split the indices of the training samples into two sets (train (80%) and validation (20%))\n",
    "    # 'splits' contains a tuple of lists ([train_indices], [validation_indices])\n",
    "    #    - stratify=True -> split the data in such a way that each class's proportion in the train and validation\n",
    "    #      datasets is approximately the same as the proportion in the original dataset\n",
    "    #    - random_state is the seed\n",
    "    splits = get_splits(train_labels, valid_size=.2, stratify=True, random_state=23, shuffle=True)\n",
    "    print(splits)\n",
    "    print(\"--> currently, the above plot wrongly labels 'Valid' as 'Test'\")\n",
    "\n",
    "    # define transformations:\n",
    "    #    - None -> no transformation to the input (X)\n",
    "    #    - Categorize() -> convert labels into categorical format; converts the labels to integers\n",
    "    # my labels are already ints, but I'll leave it here as a more general case\n",
    "    tfms  = [None, [Categorize()]]\n",
    "\n",
    "    # creates tensors to train on, e.g.,\n",
    "    #    dsets[0]: (TSTensor(vars:5, len:500, device=cpu, dtype=torch.float32), TensorCategory(0))\n",
    "    dsets = TSDatasets(train_signals, train_labels, tfms=tfms, splits=splits, inplace=True)\n",
    "\n",
    "    print(\"#train samples:\", len(dsets.train))\n",
    "    print(\"#valid samples:\", len(dsets.valid))\n",
    "\n",
    "    # data loaders: loading data in batches; batch size 64 for training and 128 for validation\n",
    "    #    - TSStandardize: batch normalization\n",
    "    #    - num_workers: 0 -> data loaded in main process\n",
    "    dls = TSDataLoaders.from_dsets(\n",
    "        dsets.train, dsets.valid, bs=[16, 32], batch_tfms=[TSStandardize()], num_workers=0\n",
    "    )\n",
    "    # vis a batch\n",
    "    dls.show_batch(nrows=3, ncols=3, sharey=True)\n",
    "    return dls\n",
    "\n",
    "def train_tsai_model(dls: TSDataLoaders, model: XCM) -> Learner:\n",
    "    # learner encapsulates the data, the model, and other details related to the training process\n",
    "    learn = Learner(dls, model, metrics=accuracy, loss_func=CrossEntropyLossFlat())\n",
    "\n",
    "    # saves curr state of learner (model + weights) to a file named stage0\n",
    "    learn.save('stage0')\n",
    "\n",
    "    # load state of model\n",
    "    learn.load('stage0')\n",
    "\n",
    "    # training over range of learning rates -- find suitable LR (or LR range)\n",
    "    #    - learning rate range where the loss decreases most effectively\n",
    "    learn.lr_find()\n",
    "\n",
    "    # 150 -> num of epochs\n",
    "    #    - involves varying the learning rate in a specific way during training\n",
    "    #    - the cyclical nature helps in faster convergence, avoids getting stuck in local minima,\n",
    "    #      and sometimes achieves better overall performance\n",
    "    #    - it provides a balance between exploring the loss landscape (with higher learning rates)\n",
    "    #    - and exploiting known good areas of the landscape (with lower learning rates)\n",
    "    learn.fit_one_cycle(300, lr_max=1e-3)\n",
    "\n",
    "    learn.save('stage1')\n",
    "    return learn\n",
    "\n",
    "def test_tsai_model(test_signals: np.ndarray, test_labels: np.ndarray, learn: Learner) -> np.float64:\n",
    "    # labeled test data\n",
    "    test_ds = TSDatasets(test_signals, test_labels, tfms=[None, [Categorize()]])\n",
    "    test_dl = dls.valid.new(test_ds)\n",
    "\n",
    "    test_probas, test_targets, test_preds = learn.get_preds(\n",
    "        dl=test_dl, with_decoded=True, save_preds=None, save_targs=None\n",
    "    )    \n",
    "    return skm.accuracy_score(test_targets, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7265c-1b7a-4040-ad39-2be1957c5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsai expects the data in a diff format: (samples, variables, length)\n",
    "\n",
    "# variables = 1 for univariate datasets and >1 for multivariate\n",
    "\n",
    "train_signals = train_signals.reshape(train_signals.shape[0], 1, train_signals.shape[1])\n",
    "test_signals = test_signals.reshape(test_signals.shape[0], 1, test_signals.shape[1])\n",
    "\n",
    "print(train_signals.shape)\n",
    "print(test_signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17b22-10a5-4a53-804e-76cecc8d85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = generate_tsai_dataset(train_signals, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a8d1-9966-46ca-8675-c533738d62e4",
   "metadata": {},
   "source": [
    "## Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfd86b-fe5f-443f-9a3c-1ac1139f662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### models trained on normalized version of RAW TS ###\n",
    "######################################################\n",
    "\n",
    "# creating InceptionTime (is a CNN) model (vars: 5 (5 channels), c: 2 (2 classes))\n",
    "# model = InceptionTime(dls.vars, dls.c)\n",
    "\n",
    "# TODO: InceptionTimePlus\n",
    "\n",
    "# TODO: XceptionTime\n",
    "\n",
    "# TODO: XceptionTimePlus\n",
    "\n",
    "# TODO: OmniScaleCNN\n",
    "\n",
    "# creating XCM\n",
    "# eXplainable Convolutional neural network for Multivariate time series classification (XCM)\n",
    "# model = XCM(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# creating XCMPlus\n",
    "# eXplainable Convolutional neural network for Multivariate time series classification (XCM)\n",
    "model = XCMPlus(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# creating FCN (CNN model)\n",
    "# model = FCN(dls.vars, dls.c)\n",
    "\n",
    "# TODO: FCNPlus\n",
    "\n",
    "# creating ResNet (CNN)\n",
    "# model = ResNet(dls.vars, dls.c)\n",
    "\n",
    "# TODO: ResNetPlus\n",
    "\n",
    "# TODO: XResNet1d\n",
    "\n",
    "# TODO: XResNet1dPlus\n",
    "\n",
    "# TODO: ResCNN\n",
    "\n",
    "# TODO: TCN\n",
    "\n",
    "# creating RNN\n",
    "# model = RNN(dls.vars, dls.c)\n",
    "\n",
    "# creating RNNPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = RNNPlus(dls.vars, dls.c)\n",
    "\n",
    "# TODO: RNNAttention\n",
    "\n",
    "# creating GRU (RNN model)\n",
    "# model = GRU(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = GRUPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUAttention (RNN model + attention)\n",
    "# model = GRUAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating LSTM (RNN model)\n",
    "# model = LSTM(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = LSTMPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMAttention (RNN model + attention)\n",
    "# model = LSTMAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating TSSequencerPlus\n",
    "# model = TSSequencerPlus(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating TransformerModel\n",
    "# model = TransformerModel(dls.vars, dls.c)\n",
    "\n",
    "# TODO: TST\n",
    "\n",
    "# TODO: TSTPlus\n",
    "\n",
    "# TODO: TSPerceiver\n",
    "\n",
    "# TODO: TSiT\n",
    "\n",
    "# TODO: PatchTST\n",
    "\n",
    "# TODO: ROCKETs category\n",
    "\n",
    "# TODO: Wavelet-based NNs category\n",
    "\n",
    "# TODO: Hybrid models category\n",
    "\n",
    "# TODO: Tabular models category\n",
    "\n",
    "#########################################\n",
    "### models trained on feature vectors ###\n",
    "#########################################\n",
    "\n",
    "# TODO: extract + select features, i.e., generate feature vectors\n",
    "\n",
    "# TODO: MLP\n",
    "\n",
    "# TODO: gMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c25c4-006a-4823-87eb-eb9ecd3619e7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff3c1c-d0b5-4bc7-9e35-50ec3266ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there should be no model, otherwise retraining!\n",
    "assert not os.path.isdir(\"export\")\n",
    "assert not os.path.isdir(\"models\")\n",
    "\n",
    "learn = train_tsai_model(dls, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ff55b-f9b1-407c-bb7e-9700ef2ec1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses -> loss development over all epochs for 'train' and 'valid'\n",
    "# final losses ->  zoomed-in view of the final epochs, focusing on loss values towards the end of training\n",
    "# accuracy -> validation accuracy of the model\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657fb3d-32e2-4a93-8bee-ee3d2689640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bb40b-bbed-4fa9-ad13-0d930138da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(nrows=3, ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484ca2b-1c15-42c4-afc7-f78753a41aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_probas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c663c8-5f6e-4476-8829-43f1e0f02a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a82e2-e38f-4836-9e67-6ed68d628bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interested in cases where the model made incorrect predictions at least 3 times\n",
    "confusions = interp.most_confused(min_val=3)\n",
    "for actual_class, pred_class, times in confusions:\n",
    "    print(\"pred:\", pred_class)\n",
    "    print(\"actual:\", actual_class)\n",
    "    print(times, \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eaf07d-6210-40bc-8171-e7149ab71f98",
   "metadata": {},
   "source": [
    "## Inference on additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a52a8-9234-4025-be04-fcc637ed726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_tsai_model(test_signals, test_labels, learn)\n",
    "print(\"test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1045db-1faa-4f32-8bfa-afc12cdc49aa",
   "metadata": {},
   "source": [
    "## GradCAM for XCM and XCMPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399a475-c7d8-4e99-b8d4-53757a992539",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(model) in [tsai.models.XCMPlus.XCMPlus, tsai.models.XCM.XCM]\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "\n",
    "print(yb[0])\n",
    "print(xb[0])\n",
    "\n",
    "model.show_gradcam(xb[0], yb[0], figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cddb87-8bc1-411b-9e9e-29c222522553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the built-in gradcam method creates plots that are sometimes unreadable, it is better to\n",
    "# visualize it with the methods from oscillogram_classification.cam\n",
    "\n",
    "# determine number of signals to be used for saliency map gen\n",
    "num_test_samples = 10\n",
    "\n",
    "# convert test data to CUDA tensors\n",
    "xb = torch.tensor(test_signals[:num_test_samples], dtype=torch.float32).to('cuda:0')\n",
    "yb = torch.tensor(test_labels[:num_test_samples], dtype=torch.float32).to('cuda:0')\n",
    "\n",
    "input_data, probabilities, targets, predictions = learn.get_X_preds(xb.cpu(), yb.cpu(), with_input=True)\n",
    "predictions = predictions.strip('][').split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5940832-a8fb-40a1-b6c3-53a830382536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_index = random.randint(0, len(xb) - 1)\n",
    "\n",
    "all_attr_maps = {}\n",
    "\n",
    "for idx in range(len(xb)):\n",
    "    if type(model) == tsai.models.XCMPlus.XCMPlus:\n",
    "        att_maps = get_attribution_map(\n",
    "            model,\n",
    "            [model.backbone.conv2dblock, model.backbone.conv1dblock],\n",
    "            xb[idx],\n",
    "            detach=True,\n",
    "            apply_relu=True\n",
    "        )\n",
    "    else:  # XCM\n",
    "        att_maps = get_attribution_map(\n",
    "            model,\n",
    "            [model.conv2dblock, model.conv1dblock],\n",
    "            xb[idx],\n",
    "            detach=True,\n",
    "            apply_relu=True\n",
    "        )\n",
    "    att_maps[0] = (att_maps[0] - att_maps[0].min()) / (att_maps[0].max() - att_maps[0].min())\n",
    "    att_maps[1] = (att_maps[1] - att_maps[1].min()) / (att_maps[1].max() - att_maps[1].min())\n",
    "\n",
    "    all_attr_maps[idx] = att_maps\n",
    "    print(\"Ground truth: \", int(yb[idx]), \" Prediction: \", predictions[idx])\n",
    "\n",
    "plot_heatmaps_as_overlay(\n",
    "    {\"var. attr. map \" + str(i) + \"(gt: \" + str(int(yb[i])) + \", pr: \" + predictions[i] + \")\": all_attr_maps[i][0].cpu().numpy()[0] for i in range(len(xb))},\n",
    "    xb.cpu().numpy()[0].flatten(),\n",
    "    'test_plot',\n",
    "    np.array(range(len(xb[0].cpu().numpy()[0])))\n",
    ")\n",
    "plot_heatmaps_as_overlay(\n",
    "    {\"time attr. map \" + str(i): all_attr_maps[i][1].cpu().numpy()[0] for i in range(len(xb))},\n",
    "    xb.cpu().numpy()[0].flatten(),\n",
    "    'test_plot',\n",
    "    np.array(range(len(xb[0].cpu().numpy()[0])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713b109-33ff-4ac6-96ce-60418f7af125",
   "metadata": {},
   "source": [
    "## Cross-validation for tsai training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca23b84-ce0a-40b5-ad96-76b90a61b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "train_test_splits = get_splits(\n",
    "    np.concatenate((train_labels, test_labels), axis=0),\n",
    "    n_splits=k,\n",
    "    valid_size=.2,\n",
    "    stratify=True,\n",
    "    random_state=23,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d39cee-069b-4859-b373-1ba36a939889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = np.concatenate((train_signals, test_signals), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "test_accuracies = []\n",
    "\n",
    "for train_test_split in train_test_splits:\n",
    "    train_split_signals = all_signals[train_test_split[0]]\n",
    "    test_split_signals = all_signals[train_test_split[1]]\n",
    "    train_split_labels = all_labels[train_test_split[0]]\n",
    "    test_split_labels = all_labels[train_test_split[1]]\n",
    "\n",
    "    # the training data will be further split into train and validation\n",
    "    dls = generate_tsai_dataset(train_split_signals, train_split_labels)\n",
    "\n",
    "    model = TransformerModel(dls.vars, dls.c)\n",
    "    learn = train_tsai_model(dls, model)\n",
    "    learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')\n",
    "    test_acc = test_tsai_model(test_split_signals, test_split_labels, learn)\n",
    "    test_accuracies.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f12b0-1026-4dd6-b14e-b922734fd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)\n",
    "print(\"Mean accuracy over all folds: \", np.mean(test_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8b0b1",
   "metadata": {},
   "source": [
    "## Load and apply already trained torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcb6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"../data/Lambdasonde.pth\"\n",
    "model = torch.load(model_path)\n",
    "# ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "tensors = torch.from_numpy(test_signals).float()\n",
    "\n",
    "# iterate over test signals\n",
    "for idx in range(tensors.shape[0]):\n",
    "    # assumes model outputs logits for a multi-class classification problem\n",
    "    logits = model(tensors[[idx]])\n",
    "    # convert logits to probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    print(probabilities)\n",
    "    first_class = float(probabilities[0][0])\n",
    "    second_class = float(probabilities[0][1])\n",
    "\n",
    "    if first_class < second_class:\n",
    "        print(\"pred POS \\t ground truth:\", test_titles[idx])\n",
    "    else:\n",
    "        print(\"pred NEG \\t ground truth:\", test_titles[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
